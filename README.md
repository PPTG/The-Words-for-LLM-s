# The Words for LLM's

![Dashboard Screenshot](screenshots/dashboard.png)

A proxy service that integrates LLMs (Large Language Models) with webhook automation, featuring keyword-based triggers and a web-based management interface.

*UsÅ‚uga proxy integrujÄ…ca modele LLM (Large Language Models) z automatyzacjÄ… webhookÃ³w, z funkcjÄ… wyzwalaczy opartych na sÅ‚owach kluczowych i webowym interfejsem zarzÄ…dzania.*

## ğŸŒ Languages / JÄ™zyki

- [English](#english)
- [Polski](#polski)

---

<a name="english"></a>
## English

### ğŸ“‹ Overview

The Words for LLM's is a proxy service that sits between your applications and LLM backends (such as Llama.cpp or Ollama). It adds powerful keyword detection capabilities to automatically trigger webhook actions based on specific words found in user queries. The system includes a web-based dashboard for easy management of keywords, configuration settings, and monitoring of service status.

### âœ¨ Features

- **LLM Proxy Service**: Compatible with Llama.cpp and Ollama backends
- **Keyword Trigger System**: Define keywords that trigger specific webhook actions
- **Webhook Integration**: Works with n8n or Flowise for automation workflows
- **OpenAI API Compatibility**: Compatible with applications that use OpenAI's API format
- **Web Dashboard**: Easy management of keywords and configuration
- **Bilingual Interface**: Available in English and Polish

### ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI (Python)
- **Frontend**: HTML, Alpine.js, TailwindCSS
- **Web Server**: Flask for serving the web interface
- **Database**: SQLite
- **Containerization**: Docker and Docker Compose

### ğŸ“Š Screenshots

![Dashboard](screenshots/dashboard.png)
*Dashboard view showing system status*

![Keywords Management](screenshots/keywords.png)
*Keywords management interface*

![Settings](screenshots/settings.png)
*System configuration settings*

### ğŸš€ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/words-for-llms.git
   cd words-for-llms
   ```

2. Configure the environment variables in `docker-compose.yml` for your setup.

3. Start the services using Docker Compose:
   ```bash
   docker-compose up -d
   ```

4. Access the web interface at `http://localhost:5555`

### ğŸ“ Configuration

The system can be configured via the web interface or by directly editing the database. Main configuration options include:

- **LLM Backend**: Choose between Llama.cpp or Ollama
- **Webhook Service**: Configure n8n or Flowise for automation
- **Model Settings**: Set temperature, max tokens, and other LLM parameters
- **Advanced Settings**: Configure timeouts and debug modes

### ğŸ” How It Works

1. User queries are sent to the proxy service
2. The system scans the messages for defined keywords
3. If a keyword is detected, the message is routed to the configured webhook
4. If no keywords match, the message is processed by the configured LLM backend
5. The response is returned to the user

### ğŸ”Œ API Endpoints

The service provides several API endpoints:
- Standard chat endpoints: `/api/chat`, `/completion`
- OpenAI-compatible endpoints: `/v1/chat/completions`, `/v1/models`
- Management endpoints: `/api/keywords`, `/api/config`, `/api/health`

### ğŸ¯ Use Cases

- Enhancing LLM responses with external data from APIs
- Creating customized responses for specific user queries
- Building automated workflows triggered by natural language
- Monitoring and analyzing user queries

---

<a name="polski"></a>
## Polski

### ğŸ“‹ PrzeglÄ…d

The Words for LLM's to usÅ‚uga proxy dziaÅ‚ajÄ…ca pomiÄ™dzy aplikacjami a backendami LLM (takimi jak Llama.cpp lub Ollama). Dodaje zaawansowane moÅ¼liwoÅ›ci wykrywania sÅ‚Ã³w kluczowych, aby automatycznie wyzwalaÄ‡ akcje webhookÃ³w na podstawie konkretnych sÅ‚Ã³w znalezionych w zapytaniach uÅ¼ytkownikÃ³w. System zawiera webowy panel administracyjny do Å‚atwego zarzÄ…dzania sÅ‚owami kluczowymi, ustawieniami konfiguracji i monitorowania stanu usÅ‚ugi.

### âœ¨ Funkcje

- **UsÅ‚uga Proxy LLM**: Kompatybilna z backendami Llama.cpp i Ollama
- **System Wyzwalaczy Opartych na SÅ‚owach Kluczowych**: Zdefiniuj sÅ‚owa kluczowe, ktÃ³re uruchamiajÄ… okreÅ›lone akcje webhookÃ³w
- **Integracja WebhookÃ³w**: WspÃ³Å‚pracuje z n8n lub Flowise dla przepÅ‚ywÃ³w automatyzacji
- **KompatybilnoÅ›Ä‡ z API OpenAI**: Kompatybilny z aplikacjami uÅ¼ywajÄ…cymi formatu API OpenAI
- **Panel Webowy**: Åatwe zarzÄ…dzanie sÅ‚owami kluczowymi i konfiguracjÄ…
- **DwujÄ™zyczny Interfejs**: DostÄ™pny w jÄ™zyku angielskim i polskim

### ğŸ› ï¸ Technologie

- **Backend**: FastAPI (Python)
- **Frontend**: HTML, Alpine.js, TailwindCSS
- **Serwer WWW**: Flask do obsÅ‚ugi interfejsu webowego
- **Baza danych**: SQLite
- **Konteneryzacja**: Docker i Docker Compose

### ğŸ“Š Zrzuty ekranu

![Dashboard](screenshots/dashboard.png)
*Widok panelu pokazujÄ…cy status systemu*

![ZarzÄ…dzanie sÅ‚owami kluczowymi](screenshots/keywords.png)
*Interfejs zarzÄ…dzania sÅ‚owami kluczowymi*

![Ustawienia](screenshots/settings.png)
*Ustawienia konfiguracji systemu*

### ğŸš€ Instalacja

1. Sklonuj repozytorium:
   ```bash
   git clone https://github.com/yourusername/words-for-llms.git
   cd words-for-llms
   ```

2. Skonfiguruj zmienne Å›rodowiskowe w pliku `docker-compose.yml` dla swojej instalacji.

3. Uruchom usÅ‚ugi za pomocÄ… Docker Compose:
   ```bash
   docker-compose up -d
   ```

4. DostÄ™p do interfejsu webowego pod adresem `http://localhost:5555`

### ğŸ“ Konfiguracja

System moÅ¼na skonfigurowaÄ‡ za pomocÄ… interfejsu webowego lub poprzez bezpoÅ›redniÄ… edycjÄ™ bazy danych. GÅ‚Ã³wne opcje konfiguracyjne obejmujÄ…:

- **Backend LLM**: WybÃ³r miÄ™dzy Llama.cpp a Ollama
- **UsÅ‚uga Webhook**: Konfiguracja n8n lub Flowise do automatyzacji
- **Ustawienia Modelu**: Ustawienie temperatury, maksymalnej liczby tokenÃ³w i innych parametrÃ³w LLM
- **Ustawienia Zaawansowane**: Konfiguracja limitÃ³w czasu i trybÃ³w debugowania

### ğŸ” Jak to dziaÅ‚a

1. Zapytania uÅ¼ytkownikÃ³w sÄ… wysyÅ‚ane do usÅ‚ugi proxy
2. System skanuje wiadomoÅ›ci w poszukiwaniu zdefiniowanych sÅ‚Ã³w kluczowych
3. JeÅ›li wykryto sÅ‚owo kluczowe, wiadomoÅ›Ä‡ jest kierowana do skonfigurowanego webhooka
4. JeÅ›li nie znaleziono pasujÄ…cych sÅ‚Ã³w kluczowych, wiadomoÅ›Ä‡ jest przetwarzana przez skonfigurowany backend LLM
5. OdpowiedÅº jest zwracana do uÅ¼ytkownika

### ğŸ”Œ Endpointy API

UsÅ‚uga udostÄ™pnia kilka endpointÃ³w API:
- Standardowe endpointy czatu: `/api/chat`, `/completion`
- Endpointy kompatybilne z OpenAI: `/v1/chat/completions`, `/v1/models`
- Endpointy zarzÄ…dzania: `/api/keywords`, `/api/config`, `/api/health`

### ğŸ¯ Przypadki uÅ¼ycia

- Wzbogacanie odpowiedzi LLM o zewnÄ™trzne dane z API
- Tworzenie niestandardowych odpowiedzi na konkretne zapytania uÅ¼ytkownikÃ³w
- Budowanie zautomatyzowanych przepÅ‚ywÃ³w pracy wyzwalanych przez jÄ™zyk naturalny
- Monitorowanie i analizowanie zapytaÅ„ uÅ¼ytkownikÃ³w

---

## ğŸ“„ License / Licencja

MIT License / Licencja MIT
